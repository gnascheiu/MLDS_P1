{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMp9kLpkKApWZ/GiBLK78cL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import random\n","import pandas as pd"],"metadata":{"id":"xEoEQc6lYKii","executionInfo":{"status":"ok","timestamp":1648438812119,"user_tz":-420,"elapsed":343,"user":{"displayName":"Thùy Dương Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJye1XD3Lr2H1XJH3Uh5F3MpBf4yu1q_LJffZwDw=s64","userId":"15681588103679775246"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["MAX_SENTENCE_LENGTH = 500\n","NUM_CLASSES = 20"],"metadata":{"id":"mIODKKDLYNL0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Model**"],"metadata":{"id":"jxKZgowLYWGM"}},{"cell_type":"code","source":["class RNN:\n","  def __init__(self,\n","               vocab_size,\n","               embedding_size,\n","               lstm_size,\n","               batch_size):\n","    self._vocab_size = vocab_size\n","    self._embedding_size = embedding_size\n","    self._lstm_size = lstm_size\n","    self._batch_size = batch_size\n","\n","    self._data = tf.placeholder(tf.int32, shape=[batch_size, MAX_SENTENCE_LENGTH])\n","    self._labels = tf.placeholder(tf.int32, shape=[batch_size, ])\n","    self._sentence_lengths = tf.placeholder(tf.int32, shape=[batch_size, ])\n","    self._final_tokens = tf.placeholder(tf.int32, shape=[batch_size, ])\n","\n","\n","  def embedding_layer(self, indices):\n","    pretrained_vectors = []\n","    pretrained_vectors.append(np.zeros(self._embedding_size))\n","    np.random.seed(2022)\n","    for _ in range (self._vocab_size + 1):\n","      pretrained_vectors.append(np.random.normal(loc=0., scale=1., size=self._embedding_size))\n","\n","    pretrained_vectors = np.array(pretrained_vectors)\n","\n","    self._embedding_matrix = tf.get_variable(\n","        name='embedding',\n","        shape=(self._vocab_size + 2, self._embedding_size),\n","        initializer=tf.constant_initializer(pretrained_vectors)\n","    )\n","    return tf.nn.embedding_lookup(self._embedding_matrix, indices)\n","\n","  def LSTM_layer(self, embeddings):\n","    lstm_cell = tf.contrib.rnn.BasicLSTMCell(self._lstm_size)\n","    zero_state = tf.zeros(shape=(self._batch_size, self._lstm_size))\n","    initial_state = tf.contrib.rnn.LSTMStateTuple(zero_state, zero_state)\n","    lstm_inputs = tf.unstack(\n","        tf.transpose(embeddings, perm=[1, 0, 2])\n","    )\n","    lstm_outputs, last_state = tf.nn.static_rnn(\n","        cell=lstm_cell,\n","        inputs=lstm_inputs,\n","        initial_state=initial_state,\n","        sequence_length=self._sentence_lengths\n","    )\n","    lstm_outputs = tf.unstack(\n","        tf.transpose(lstm_outputs, perm=[1, 0, 2])\n","    )\n","    lstm_outputs = tf.concat(\n","        lstm_outputs,\n","        axis=0\n","    )\n","    mask = tf.sequence_mask(\n","        lengths=self._sentence_lengths,\n","        maxlen=MAX_SENTENCE_LENGTH,\n","        dtype=tf.float32\n","    ) \n","    mask = tf.concat(tf.unstack(mask, axis=0), axis=0)\n","    mask = tf.expand_dims(mask, -1)\n","\n","    lstm_outputs = mask * lstm_outputs\n","    lstm_outputs_split = tf.split(lstm_outputs, num_or_size_splits=self._batch_size)\n","    lstm_outputs_sum = tf.reduce_sum(lstm_outputs_split, axis=1)\n","    lstm_outputs_average = lstm_outputs_sum / tf.expand_dims(\n","        tf.cast(self._sentence_lengths, tf.float32),\n","        -1\n","    ) \n","\n","    return lstm_outputs_average\n","        \n","  def build_graph(self):\n","    embeddings = self.embedding_layer(self._data)\n","    lstm_outputs = self.LSTM_layer(embeddings)\n","\n","    weigths = tf.get_variable(\n","        name = 'final_layer_weights',\n","        shape = (self._lstm_size, NUM_CLASSES),\n","        initializer = tf.random_normal_initializer(seed = 2022)\n","    )\n","    biases = tf.get_variable(\n","        name = 'final_layer_biases',\n","        shape = (NUM_CLASSES),\n","        initializer = tf.random_normal_initializer(seed = 2022)\n","    )\n","    logits = tf.matmul(lstm_outputs, weigths) + biases\n","\n","    labels_one_hot = tf.one_hot(\n","        indices = self._labels,\n","        depth = NUM_CLASSES,\n","        dtype = tf.float32\n","    )\n","\n","    loss = tf.nn.softmax_cross_entropy_with_logits(\n","        labels = labels_one_hot,\n","        logits = logits\n","    )\n","    loss = tf.reduce_mean(loss)\n","\n","    probs = tf.nn.softmax(logits)\n","    predicted_labels = tf.argmax(probs, axis = 1)\n","    predicted_labels = tf.squeeze(predicted_labels)\n","    return predicted_labels, loss\n","\n","  def trainer(self, loss, learning_rate):\n","    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n","    return train_op\n"],"metadata":{"id":"m_YUUKcRYbe9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **DataReader**"],"metadata":{"id":"fUxRJonZYfsR"}},{"cell_type":"code","source":["class DataReader:\n","  def __init__(self, data_path, batch_size):\n","    self._batch_size = batch_size\n","    with open(data_path) as f:\n","      d_lines = f.read().splitlines()\n","\n","    self._data = []\n","    self._labels = []\n","    self._sentence_lengths = []\n","    self._final_tokens = [] \n","    for data_id, line in enumerate(d_lines):\n","      features = line.split('<fff>')\n","      label, doc_id, sentence_length = int(features[0]), int(features[1]), int(features[2])\n","      tokens = features[3].split()\n","\n","      self._data.append(tokens)\n","      self._sentence_lengths.append(sentence_length)\n","      self._labels.append(label)\n","      self._final_tokens.append(tokens[-1])\n","\n","    self._data = np.array(self._data)\n","    self._labels = np.array(self._labels)\n","    self._sentence_lengths = np.array(self._sentence_lengths)\n","    self._final_tokens = np.array(self._final_tokens)\n","\n","    self._num_epoch = 0\n","    self._batch_id = 0\n","    self._size = len(self._data)\n","\n","  def next_batch(self):\n","    start = self._batch_id * self._batch_size\n","    end = start + self._batch_size\n","    self._batch_id += 1\n","\n","    if end + self._batch_size > len(self._data):\n","      self._size = end\n","      end = len(self._data)\n","      start = end - self._batch_size\n","      self._num_epoch += 1\n","      self._batch_id = 0\n","      indices = range(len(self._data))\n","      random.seed(2022)\n","      random.shuffle(indices)\n","      self._data, self._labels, self._sentence_lengths, self._final_tokens = self._data[indices], self._labels[indices], self._sentence_lengths[indices], self._final_tokens[indices]\n","\n","    return self._data[start:end], self._labels[start:end], self._sentence_lengths[start:end]"],"metadata":{"id":"sURUwBErYiRB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Training**"],"metadata":{"id":"XMbOzuP5aHoM"}},{"cell_type":"code","source":["loss_report = []\n","accuracy_report = []"],"metadata":{"id":"g_YJrFA1ljGE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_and_evaluate_RNN():\n","  with open('/content/vocab-raw.txt') as f:\n","    vocab_size = len(f.read().splitlines())\n","\n","  tf.set_random_seed(2022)\n","  rnn = RNN(\n","      vocab_size=vocab_size,\n","      embedding_size=300,\n","      lstm_size=50,\n","      batch_size=50\n","  )\n","  predicted_labels, loss = rnn.build_graph()\n","  train_op = rnn.trainer(loss=loss, learning_rate=0.01)\n","\n","  with tf.Session() as sess:\n","    train_data_reader = DataReader(\n","        data_path='/content/20news-train-encoded.txt',\n","        batch_size=50\n","    )\n","    test_data_reader = DataReader(\n","        data_path='/content/20news-test-encoded.txt',\n","        batch_size=50\n","    )\n","    step = 0\n","    MAX_STEP = 10000\n","\n","    sess.run(tf.global_variables_intializer())\n","    while step < MAX_STEP:\n","      next_train_batch = train_data_reader.next_batch()\n","      train_data, train_labels, train_sentence_lengths, train_final_tokens = next_train_batch\n","      plabels_eval, loss_eval, _ = sess.run(\n","          [predicted_labels, loss, train_op],\n","          feed_dict={\n","              rnn._data: train_data,\n","              rnn._labels: train_labels,\n","              rnn._sentence_lengths: train_sentence_lengths,\n","              rnn._final_tokens: train_final_tokens\n","          }\n","      )\n","      step += 1\n","      if step % 20 == 0:\n","        loss_report.append(loss_eval)\n","        print ('loss:', loss_eval)\n","      if train_data_reader._batch_id == 0:\n","        num_true_preds = 0\n","        while True:\n","          next_test_batch = test_data_reader.next_batch()\n","          test_data, test_labels, test_sentence_lengths, test_final_tokens = next_test_batch\n","\n","          test_plabels_eval = sess.run(\n","              predicted_labels,\n","              feed_dict={\n","                  rnn._data: test_data,\n","                  rnn._labels: test_labels,\n","                  rnn._sentence_lengths: test_sentence_lengths,\n","                  rnn._final_tokens: test_final_tokens\n","              }\n","          )\n","          matches = np.equal(test_plabels_eval, test_labels)\n","          num_true_preds += np.sum(matches.astype(float))\n","\n","          if test_data_reader._batch_id == 0:\n","            break\n","\n","        accuracy_report.append(num_true_preds * 100. / test_data_reader._size)\n","        print ('Epoch:', train_data_reader._num_epoch)\n","        print ('Accuracy on test data:', num_true_preds * 100. / len(test_data_reader._data))\n"],"metadata":{"id":"NtnYFWtGaKDD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_and_evaluate_RNN()\n","\n","  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358},"id":"yTKt5yl7hZtS","executionInfo":{"status":"error","timestamp":1648438824947,"user_tz":-420,"elapsed":334,"user":{"displayName":"Thùy Dương Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJye1XD3Lr2H1XJH3Uh5F3MpBf4yu1q_LJffZwDw=s64","userId":"15681588103679775246"}},"outputId":"b28e9ef0-c232-4008-ba18-e5d7eab18f40"},"execution_count":38,"outputs":[{"output_type":"error","ename":"UnicodeDecodeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-90f57ef5c8bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_evaluate_RNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-31-b9756bdc97b8>\u001b[0m in \u001b[0;36mtrain_and_evaluate_RNN\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_evaluate_RNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/vocab-raw.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2022\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xba in position 154847: invalid start byte"]}]}]}