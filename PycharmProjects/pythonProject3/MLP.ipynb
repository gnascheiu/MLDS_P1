{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7554,"status":"ok","timestamp":1647960744891,"user":{"displayName":"Thùy Dương Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJye1XD3Lr2H1XJH3Uh5F3MpBf4yu1q_LJffZwDw=s64","userId":"15681588103679775246"},"user_tz":-420},"id":"7O2Yl9I71oba","outputId":"ccd438bf-2020-4415-b177-5cc8d3b3ed0c"},"outputs":[],"source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","from sklearn.linear_model import LogisticRegression\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptOL4O7j15zP"},"outputs":[],"source":["class MLP:\n","  def __init__(self, vocab_size, hidden_unit, number_classer):\n","    super(MLP, self).__init__()\n","    self.vocab_size = vocab_size  \n","    self.hidden_unit = hidden_unit  \n","    self.NUMBER_CLASSES = number_classer  \n","\n"," # Build model and return loss function, label predict  \n","  def build_model(self):\n","    self.X = tf.placeholder(tf.float32, shape=[None, self.vocab_size])\n","    self.Y = tf.placeholder(tf.int32, shape=[None, ])\n","\n","    # weights and bias are variables to be updated\n","    weights_1 = tf.get_variable(name='weights_input_hidden', \n","                                shape=(self.vocab_size, self.hidden_unit),\n","                                initializer=tf.random_normal_initializer(2018))\n","    biases_1 = tf.get_variable(name='biases_input_hidden',\n","                               shape=(self.hidden_unit),\n","                               initializer=tf.random_normal_initializer(2018))\n","    weights_2 = tf.get_variable(name='weights_hidden_output',\n","                                shape=(self.hidden_unit, self.NUMBER_CLASSES),\n","                                initializer=tf.random_normal_initializer(2018))\n","    biases_2 = tf.get_variable(name='biases_hidden_output',\n","                               shape=(self.NUMBER_CLASSES),\n","                               initializer=tf.random_normal_initializer(2018))\n","    \n","    # Calculate value of hidden layer\n","    hidden_units = tf.sigmoid(tf.matmul(self.X, weights_1) + biases_1)\n","    # Calculate value of output\n","    logit = tf.matmul(hidden_units, weights_2) + biases_2\n","    # Convert Y to form one hot\n","    label_one_hot = tf.one_hot(\n","            indices=self.Y, depth=self.NUMBER_CLASSES, dtype=tf.float32)\n","    # Calculate loss with cross entropy\n","    loss = tf.nn.softmax_cross_entropy_with_logits(\n","            labels=label_one_hot, logits=logit)\n","    loss = tf.reduce_mean(loss)\n","\n","    # Predict label from output\n","    predict_label = tf.squeeze(tf.argmax(tf.nn.softmax(logit), axis=1))\n","\n","    return predict_label, loss\n","\n","  # Set optimal algorithm\n","  def trainer(self, loss, learning_rate):\n","    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n","    return train_op"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PxXjt9ff8mDd"},"outputs":[],"source":["class DataReader(object):\n","\n","  def __init__(self, path_file, batch_size, vocab_size):\n","    super(DataReader, self).__init__()\n","    self.path_file = path_file\n","    self.vocab_size = vocab_size\n","    self.batch_size = batch_size\n","    self.data = []\n","    self.label = []\n","\n","    with open(path_file) as f:\n","      data = f.read().split(\"\\n\")\n","    for line in data:\n","      vector_line = [0.0 for _ in range(vocab_size)]\n","      feature = line.split(\"<fff>\")\n","      self.label.append(int(feature[0]))\n","      for word in feature[2].split():\n","        vector_line[int(word.split(':')[0])] = float(word.split(':')[1])\n","        # vector line, value of index is tf-idf value\n","      self.data.append(vector_line)\n","   \n","    self.data = np.array(self.data)\n","    self.label = np.array(self.label)\n","  \n","    self.num_epoch = 0\n","    self.batch_id = 0\n","  \n","  def next_batch(self):\n","    start = self.batch_id * self.batch_size\n","    end = start + self.batch_size\n","    #if call next batch, batch_id +1\n","    self.batch_id += 1\n","\n","    if ((end + self.batch_size) > len(self.data)):\n","      end = len(self.data)\n","      self.num_epoch += 1\n","      self.batch_id = 0\n","      indices = [index for index in range(len(self.data))]\n","      random.seed(2018)\n","      random.shuffle(indices)\n","      self.data, self.label = self.data[indices], self.label[indices]\n","    \n","    return self.data[start:end], self.label[start:end]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8B30JQYOaG8"},"outputs":[],"source":["def train(vocab_size, hidden_unit, number_classer, learning_rate, epoch, path_train, path_test):\n","  print(\"------load data train------\")\n","  train_reader = DataReader(path_file=path_train, batch_size=128, vocab_size=vocab_size)\n","  print(\"------load data test------\")\n","  test_reader = DataReader(path_file=path_test, batch_size=128, vocab_size=vocab_size)\n","  print(\"------load finish------\")\n","  \n","  #creat model MLP\n","  mlp = MLP(vocab_size=vocab_size, hidden_unit=hidden_unit, number_classer=number_classer)\n","  predict_label, loss = mlp.build_model()\n","  train_opt = mlp.trainer(loss, learning_rate)\n","\n","  index_epoch = []\n","  loss_of_epoch =[]\n","  accuracy_epoch =[]\n","\n","  #creat session\n","  with tf.Session() as sess:\n","    epoch, MAX_EPOCH = 0, epoch\n","    sess.run(tf.global_variables_initializer())\n","    while epoch < MAX_EPOCH:\n","      loss_print = 0\n","      if train_reader.batch_id == 0:\n","        count =1\n","      while count != 0:\n","        train_data, train_label = train_reader.next_batch()\n","        count = train_reader.batch_id\n","        _, loss_, _ = sess.run([predict_label, loss, train_opt], feed_dict={\n","            mlp.X: train_data,\n","            mlp.Y: train_label})\n","        loss_print = loss_\n","      epoch += 1\n","      print(\"------ epoch {} ------- loss {}\".format(epoch, loss_print))\n","      index_epoch.append(epoch)\n","      loss_of_epoch.append(loss_print)\n","      #get accuracy of epoch\n","      num_true_preds = 0\n","      test_plabels_eval = sess.run(predict_label,\n","                                   feed_dict={\n","                                       mlp.X: test_reader.data,\n","                                       mlp.Y: test_reader.label})\n","      matches = np.equal(test_plabels_eval, test_reader.label)\n","      num_true_preds = np.mean(matches)\n","      acc = num_true_preds / len(test_reader.data)\n","      accuracy_epoch.append(acc)\n","      print(\"------Accuracy------\", acc)\n","    #save parameters\n","    trainable_variables = tf.trainable_variables()\n","    for variable in trainable_variables:\n","      save_paramenters(name=variable.name, value=variable.eval(), epoch=epoch)\n","  plt.plot(index_epoch, loss_of_epoch)\n","  plt.plot(index_epoch, accuracy_epoch)\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLn0RI8ESJOw"},"outputs":[],"source":["def save_paramenters(name, value, epoch):\n","  filename = name.replace(':', '-') + '-epoch-{}.txt'.format(epoch)\n","  # if length value shape = 1 => bias\n","  # else => weight\n","  if(len(value.shape) == 1):\n","    string_form = ','.join([str(number) for number in value])\n","  else:\n","    string_form = '\\n'.join([','.join([str(number) for number in value[\n","                                row]]) for row in range(value.shape[0])])\n","  with open('./save_paras/' + filename, 'w') as f:\n","    f.write(string_form)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8qVF3OGSYkn"},"outputs":[],"source":["def restore_paramenters(name, epoch):\n","  filename = name.replace(':', '-') + '-epoch-{}.txt'.format(epoch)\n","  with open('./save_paras/' + filename, 'r') as f:\n","    list_line = f.read().split('\\n')\n","  # if number of line = 1 => bias\n","  # else => weight\n","  if(len(list_line) == 1):\n","    value = [float(number) for number in list_line[0].split(',')]\n","  else:\n","    value = [[float(number) for number in list_line[index].split(',')]\n","                 for index in range(len(list_line))]\n","  return value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GOhSVWMoSkI_"},"outputs":[],"source":["def test_model(vocab_size, hidden_unit, number_classer, epoch, path_test):\n","  # function get accuracy model trained\n","  mlp = MLP(vocab_size=vocab_size, hidden_unit=hidden_unit, number_classer=number_classer)\n","  predict_label, loss = mlp.build_model()\n","  # read data test\n","  test_reader = DataReader(path_file=path_test, batch_size=50, vocab_size=vocab_size)\n","  with tf.Session() as sess:\n","    trainable_variables = tf.trainable_variables()\n","    for variable in trainable_variables:\n","      saved_value = restore_paramenters(variable.name, epoch=epoch)\n","      assign_op = variable.assign(saved_value)\n","      sess.run(assign_op)\n","    num_true_preds = 0\n","    # loop data test and predict\n","    test_data, test_label = test_reader.next_batch()\n","    test_plabels_eval = sess.run(predict_label, \n","                                 feed_dict={\n","                                     mlp.X: test_data,\n","                                     mlp.Y: test_label})\n","    matches = np.equal(test_plabels_eval, test_label)\n","    # num_true_preds  is number of predict label true\n","    num_true_preds += np.sum(matches.astype(float))\n","    print('------Accuracy ', num_true_preds / len(test_reader.data))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":778},"executionInfo":{"elapsed":45354,"status":"error","timestamp":1647960811720,"user":{"displayName":"Thùy Dương Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJye1XD3Lr2H1XJH3Uh5F3MpBf4yu1q_LJffZwDw=s64","userId":"15681588103679775246"},"user_tz":-420},"id":"R7K1xwcQTI7C","outputId":"d7909ae8-5836-49db-8d93-309c61d4882b"},"outputs":[],"source":["if __name__ == '__main__':\n","  train(vocab_size=14140, hidden_unit=10, number_classer=20, learning_rate=0.0001, epoch=5,\n","        path_train=\"/data_train_tf_idf.txt\", path_test=\"/data_test_tf_idf.txt\")\n","    # test_model(vocab_size=14140, hidden_unit=100, number_classer=20,\n","    # epoch=10, path_test=\"/data_tf_idf.txt\")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOErCiTMS1txzJNdxiZt64Q","collapsed_sections":[],"name":"MLP.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
