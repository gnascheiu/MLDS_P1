{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kmean.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "yurge0BDLJx4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_clusters = 20\n",
        "data_path = '/content/data_tf_idf.txt'\n",
        "vocab_size = 14140"
      ],
      "metadata": {
        "id": "a0mS1JNLm-C2"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Member():\n",
        "  def __init__(self, r_d, label=None, doc_id=None):\n",
        "    self._r_d = r_d \n",
        "    self._label = label\n",
        "    self._doc_id = doc_id"
      ],
      "metadata": {
        "id": "7oKC9AXxLdkO"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Cluster():\n",
        "  def __init__(self):\n",
        "    self._centroid = None\n",
        "    self._members = []\n",
        "  \n",
        "  def reset_members(self):\n",
        "    self._members = []\n",
        "\n",
        "  def add_member(self, member):\n",
        "    self._members.append(member)"
      ],
      "metadata": {
        "id": "H29KkMnAL6kK"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Kmeans():\n",
        "  def __init__(self):\n",
        "    self._num_clusters = num_clusters\n",
        "    self._clusters = [Cluster() for _ in range(self._num_clusters)]\n",
        "    self._E = [] #list of centroids\n",
        "    self._S = 0  # overall similarity\n",
        "\n",
        "  def load_data(self, data_path):\n",
        "    def sparse_to_dense(sparse_r_d, vocab_size):\n",
        "      r_d = [0.0 for _ in range(vocab_size)]\n",
        "      indices_tfidfs = sparse_r_d.split()\n",
        "      for index_tfidf in indices_tfidfs:\n",
        "        index = int(index_tfidf.split(':')[0])\n",
        "        tfidf = float(index_tfidf.split(':')[1])\n",
        "        r_d[index] = tfidf \n",
        "      return np.array(r_d)\n",
        "    with open(data_path) as f:\n",
        "      d_lines = f.read().splitlines()\n",
        "    with open('/content/words_idfs.txt') as f:\n",
        "      vocab_size = len(f.read().splitlines())\n",
        "    \n",
        "    self._data = []\n",
        "    self._label_count = defaultdict(int)\n",
        "    for data_id, d in enumerate(d_lines):\n",
        "      features = d.split('<fff>')\n",
        "      label, doc_id = int(features[0]), int(features[1])\n",
        "      self._label_count[label] += 1\n",
        "      r_d = sparse_to_dense(sparse_r_d=features[2], vocab_size=vocab_size)\n",
        "      self._data.append(Member(r_d=r_d, label=label, doc_id=doc_id))\n",
        "\n",
        "\n",
        "  def random_init(self, seed_value):\n",
        "     index = []\n",
        "     i = 0\n",
        "     random.shuffle(self._data)\n",
        "     for member in self._data:\n",
        "        if (member._label not in index):\n",
        "            self._clusters[i]._label = member._label\n",
        "            self._clusters[i]._centroid = member._r_d\n",
        "            i += 1\n",
        "            index.append(member._label)\n",
        "  \n",
        "  def compute_similarity(self, member, centroid):\n",
        "     # calculate norm 2 of (member embed - centroid)\n",
        "        euclid_dist = np.linalg.norm(\n",
        "            member._r_d - centroid)\n",
        "        return euclid_dist\n",
        "\n",
        "  def select_cluster_for(self, member):\n",
        "    best_fit_cluster = None\n",
        "    max_similarity = -1\n",
        "    for cluster in self._clusters:\n",
        "      similarity = self.compute_similarity(member, cluster._centroid)\n",
        "      if similarity > max_similarity:\n",
        "        best_fit_cluster = cluster\n",
        "        max_similarity = similarity\n",
        "      \n",
        "      best_fit_cluster.add_member(member)\n",
        "      return  max_similarity\n",
        "  \n",
        "  def update_centroid_of(self, cluster):\n",
        "    # compute new centroid to cluster after each epoch\n",
        "    member_r_ds = [member._r_d for member in cluster._members]\n",
        "    aver_r_d = np.mean(member_r_ds, axis=0)\n",
        "    sqrt_sum_sqr = np.sqrt(np.sum(aver_r_d ** 2))\n",
        "    # include normal data\n",
        "    new_centroid = aver_r_d / sqrt_sum_sqr \n",
        "    cluster._centroid = new_centroid\n",
        "\n",
        "  def stopping_condition(self, criterion, threshold):\n",
        "    criteria = ['centroid', 'similarity', 'max_iters']\n",
        "    assert criterion in criteria\n",
        "    if criterion == 'max_iters':\n",
        "      if self._iteration >= threshold:\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "    elif criterion == 'centroid':\n",
        "      E_new = [list(cluster._centroid) for cluster in self._clusters]\n",
        "      E_new_minus_E = [centroid for centroid in E_new\n",
        "                       if centroid not in self._E]\n",
        "      self._E = E_new\n",
        "      if len(E_new_minus_E) <= threshold:\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "    else:\n",
        "      new_S_minus_S = self._new_S - self._S\n",
        "      self._S = self._new_S\n",
        "      if new_S_minus_S <= threshold:\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "\n",
        "  \n",
        "  def run(self, seed_value, criterion, threshold):\n",
        "    self.random_init(seed_value)\n",
        "\n",
        "    # continually update clusters until convergence\n",
        "    self._iteration = 0\n",
        "    while True:\n",
        "      # reset clusters, retain only centroids\n",
        "      for cluster in self._clusters:\n",
        "        cluster.reset_members()\n",
        "      self._new_S = 0\n",
        "      for member in self._data:\n",
        "        max_s = self.select_cluster_for(member)\n",
        "        self._new_S += max_s\n",
        "      for cluster in self._clusters:\n",
        "        self.update_centroid_of(cluster)\n",
        "      \n",
        "      self._iteration += 1\n",
        "      if self.stopping_condition(criterion, threshold):\n",
        "        break\n",
        "  \n",
        "  def compute_purity(self):\n",
        "    majority_sum = 0\n",
        "    for cluster in self._clusters:\n",
        "      member_labels = [member._label for member in cluster._members]\n",
        "      max_count = max([member_labels.count(label) for label in range(20)])\n",
        "      majority_sum += max_count\n",
        "    return majority_sum * 1. / len(self._data)\n",
        "  \n",
        "  def compute_NMI(self):\n",
        "    I_value, H_omega, H_C, N = 0., 0., 0., len(self._data)\n",
        "    for cluster in self._clusters:\n",
        "      wk = len(cluster._members) * 1.\n",
        "      H_omega += -wk / N * np.log10(wk / N)\n",
        "      member_labels = [member._label for member in cluster._members]\n",
        "      for label in range(20):\n",
        "        wk_cj = member_labels.count(label) * 1.\n",
        "        cj = self._label_count[label]\n",
        "        I_value += wk_cj / N * np.log10(N * wk_cj / (wk * cj) + 1e-12)\n",
        "      for label in range(20):\n",
        "        cj = self._label_count[label] * 1.\n",
        "        H_C += -cj / N * np.log10(cj / N)\n",
        "        return I_value * 2. / (H_omega + H_C)\n"
      ],
      "metadata": {
        "id": "2oPHoLdFMd8Y"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    kmean = Kmeans()\n",
        "    kmean.load_data(data_path)\n",
        "    kmean.run(seed_value=0, criterion='similarity', threshold=10)\n",
        "    print(kmean.compute_purity(), kmean.compute_NMI())"
      ],
      "metadata": {
        "id": "iVwqBbkjXaNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e4a90b5-adc3-4104-b43f-5b5061be0e7d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0530316422131872 1.491952763251981e-11\n"
          ]
        }
      ]
    }
  ]
}